---
title: "Practical Machine Learning Final Project"
author: "Thomas MARTEL"
date: "09/08/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

**Background**

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

**Data**

The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

**Goal**

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.


## Libraries

```{r}
library(caret)
library(rattle)
library(transport)
library(gbm)
library(rpart)
library(randomForest)
```


## Loading our Data
First of all, we should load our data and then take a first look at it.

```{r}
Train_Data<- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),header=TRUE)
dim(Train_Data)
```

```{r}
Test_Data<- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),header=TRUE)
dim(Test_Data)
```

```{r}
str(Train_Data)
```

We can observe that there are 19622 observations and 160 columns in the training data set which is provided to us. But, most of all, the first seven columns are about information users and timestamps, and many of the other columns contain NA values: we will not take those columns as features for our prediction work.

## Cleaning the training and the test data sets.

So now, the purpose is to remove from the training data set the first seven columns, and also columns with more than 70% NA values (which is an arbitrary value).

```{r}
uselessColumns <- which(colSums(is.na(Train_Data) |Train_Data=="")>0.7*dim(Train_Data)[1]) 
Train_Good_Data <- Train_Data[,-uselessColumns]
Train_Good_Data <- Train_Good_Data[,-c(1:7)]
dim(Train_Good_Data)
```

So we will keep only 53 features for our prediction model and now we will apply the same method on the test set, excepted the fact that we will keep the first seven columns in order to still be able to identify the users associated to our test data set.

```{r}
uselessColumns <- which(colSums(is.na(Test_Data) |Test_Data=="")>0.7*dim(Test_Data)[1]) 
Test_Good_Data <- Test_Data[,-uselessColumns]
dim(Test_Good_Data)
```


So now, our last step before begginning to explore the application of machine learning algorithms on our data is to divide our Train_Good_Data set.

```{r}
set.seed(123456789)
inTrain <- createDataPartition(Train_Good_Data$classe, p=0.75, list=FALSE)
Train <- Train_Good_Data[inTrain,]
Test <- Train_Good_Data[-inTrain,]
dim(Train)
```

```{r}
dim(Test)
```

Now, we are ready to experiment three machine learning algorithms: gradient boosting method, classification tree and random forests.
In order to improve our results, we will use cross-validation with 3 folds, which appears to us as a good compromise between accuracy and run speed (considereing the use of a computer with poor performances).


## Gradient Boosting Method


```{r}
trControl <- trainControl(method="cv", number=3)
model_Gradient <- train(classe~., data=Train, method="gbm", trControl=trControl, verbose=FALSE)
```

```{r}
print(model_Gradient)
```

```{r}
predictions <- predict(model_Gradient,newdata=Test)

confusion_Matrix_Gradient <- confusionMatrix(Test$classe,predictions)
confusion_Matrix_Gradient$overall
```

We obtain an accuracy of 96.50%, which is an high result, but we still need to see others in order to interpret it better.


## Classification trees

```{r}
model_Classification_Tree <- train(classe~., data=Train, method="rpart", trControl=trControl)
fancyRpartPlot(model_Classification_Tree$finalModel)
```


```{r}
predictions <- predict(model_Classification_Tree,newdata=Test)

confusion_Matrix_Tree <- confusionMatrix(Test$classe,predictions)
confusion_Matrix_Tree$table
confusion_Matrix_Tree$overall
```

We obtain an accuracy of 49.34%, which is very low in general and most of all considereing the results from the Gradient Descent Method.


## Random Forests


```{r}
model_Random_Forests <- train(classe~., data=Train, method="rf", trControl=trControl, verbose=FALSE)
print(model_Random_Forests)
```

```{r}
predictions <- predict(model_Random_Forests,newdata=Test)

confusion_Matrix_Random_Forests <- confusionMatrix(Test$classe,predictions)
confusion_Matrix_Random_Forests$table
confusion_Matrix_Random_Forests$overall
```


We obtain an accuracy of 99.14%, which is by far our best result,and we can add that this result is obtain with only 27 predictors. So, Random Forests will be the chosen method for our test.


## Conclusion

```{r}
Last_Predictions <- predict(model_Random_Forests,newdata=Test_Good_Data)
Last_Predictions
```


